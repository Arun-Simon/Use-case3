import os 
import re
from collections import Counter

log_file="app.log"
min_word_length=4
rare_threshold=2

def clean_and_tokenize(text):
    words=re.findall(r"\b[a-zA-Z]{%d,}\b"%min_word_length,text.lower())
    return words

def analyze_log(file_path):
    if not os.path.exists(file_path):
        print("Log file not found")
        return
    
    with open(file_path,"r", encoding="utf-8") as f:
        lines=f.readlines()
    
    all_words=[]
    for line in lines:
        all_words.extend(clean_and_tokenize(line))
    
    word_counts=Counter(all_words)
    rare_words={word for word, count in word_counts.items() if count <= rare_threshold}

    anomalous_lines=[]
    for line in lines:
        words=clean_and_tokenize(line)
        if any(word in rare_words for word in words):
            anomalous_lines.append(line.strip())
    
    print(f"Total lines: {len(lines)}")
    print(f"Total unique words: {len(word_counts)}")
    print(f"Rare words: {len(rare_words)}")
    print(f"Anomalous lines:{len(anomalous_lines)}")
    print("Sample anomalous lines:")
    for line in anomalous_lines[:5]:
        print(line)

if __name__=="__main__":
    analyze_log(log_file)
